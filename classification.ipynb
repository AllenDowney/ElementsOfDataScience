{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Elements of Data Science\n",
    "\n",
    "by [Allen Downey](https://allendowney.com)\n",
    "\n",
    "[MIT License](https://opensource.org/licenses/MIT)\n",
    "\n",
    "### Goals\n",
    "\n",
    "The primary goal of this notebook is to review ways to evaluate binary classification algorithms.  We'll start with the confusion matrix and the metrics that are derived from it, including accuracy, sensitivity, specificity, predictive value, false positive rate, and false negative rate.\n",
    "\n",
    "Then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Bias\n",
    "\n",
    "As an example of a classification problem, I will replicate the analysis reported in\n",
    "\"[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\", by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner, and published by [ProPublica](https://www.propublica.org), in May 2016.\n",
    "\n",
    "This article is about a statistical tool called COMPAS which was used by judges in Broward County, Florida, to inform their sentencing decisions, specifically which convicted defendents would be put in prison and which would be released on parole.\n",
    "\n",
    "COMPAS uses information about the defendants to generate a \"risk score\" which is intended to quantify the risk that defendant would commit another crime is released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of the ProPublica article used public data to assess the accuracy of those risk scores.  They explain:\n",
    "\n",
    "> We obtained the risk scores assigned to more than 7,000 people arrested in Broward County, Florida, in 2013 and 2014 and checked to see how many were charged with new crimes over the next two years, the same benchmark used by the creators of the algorithm.\n",
    "\n",
    "[In their notebook](https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb), they explain in more detail:\n",
    "\n",
    "> We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    ">\n",
    "> [...]\n",
    ">\n",
    "> Next, we sought to determine if a person had been charged with a new crime subsequent to crime for which they were COMPAS screened. We did not count traffic tickets and some municipal ordinance violations as recidivism. We did not count as recidivists people who were arrested for failing to appear at their court hearings, or people who were later charged with a crime that occurred prior to their COMPAS screening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They use this data to evaluate how well COMPAS predicts the risk that a defendant will be charged with another crime within two years of their release.\n",
    "\n",
    "Among their findings, they report:\n",
    "\n",
    "> ... the algorithm was somewhat more accurate than a coin flip. Of those deemed likely to re-offend, 61 percent were arrested for any subsequent crimes within two years.\n",
    "> \n",
    "> ... In forecasting who would re-offend, the algorithm made mistakes with black and white defendants at roughly the same rate but in very different ways.\n",
    ">\n",
    "> * The formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.\n",
    ">\n",
    "> * White defendants were mislabeled as low risk more often than black defendants.\n",
    "\n",
    "I'll start by replicating their analysis, and use this example to explain the metrics used to assess binary classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Thank you to ProPublica and the authors of \"Machine Bias\" for making their data and analysis freely available.  They are a model of open science.\n",
    "\n",
    "[This repository](https://github.com/propublica/compas-analysis) the data and analysis pipeline described on [this web page](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm).\n",
    "\n",
    "The terms of use for the data [are here](https://www.propublica.org/datastore/terms).  In compliance with those terms, I am not redistributing the data.\n",
    "\n",
    "The following cell downloads the data file we'll use directly from their repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('compas-scores-two-years.csv'):\n",
    "    !wget https://github.com/propublica/compas-analysis/raw/master/compas-scores-two-years.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells read the data file and display the column headings.  I have not found documentation for the columns in this dataset, so we'll have to infer what they mean based on the column names and how they are used in the original analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7214, 53)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = pd.read_csv(\"compas-scores-two-years.csv\")\n",
    "cp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes 7214 rows, one for each defendant, and 53 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
       "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
       "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
       "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
       "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
       "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
       "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
       "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
       "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
       "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
       "       'decile_score.1', 'score_text', 'screening_date',\n",
       "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
       "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
       "       'start', 'end', 'event', 'two_year_recid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the libraries and functions I'll use for my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate(**options):\n",
    "    \"\"\"Decorate the current axes.\n",
    "    \n",
    "    Call decorate with keyword arguments like\n",
    "    decorate(title='Title',\n",
    "             xlabel='x',\n",
    "             ylabel='y')\n",
    "             \n",
    "    The keyword arguments can be any of the axis properties\n",
    "    https://matplotlib.org/api/axes_api.html\n",
    "    \"\"\"\n",
    "    plt.gca().set(**options)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confusion Matrix\n",
    "\n",
    "The authors of \"Machine Bias\" describe their analysis in more detail in [this article](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm).  It includes this table, which summarizes many of the results they report:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/AllenDowney/ElementsOfDataScience/master/machine_bias_table.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table summarizes results for all defendants and two subgroups: defendants classified as white (\"Caucasian\" in the original dataset) and black (\"African-American\").\n",
    "\n",
    "For each group, the summary includes a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) and a number of metrics, including\n",
    "\n",
    "* FP rate: false positive rate\n",
    "* FN rate: false negative rate\n",
    "* PPV: positive predictive value\n",
    "* NPV: negative predictive value\n",
    "* LR+: positive likelihood ratio \n",
    "* LR-: negative likelihood ratio \n",
    "\n",
    "I will explain what these metrics mean and how to compute them, and we'll replicate the results in this table.\n",
    "\n",
    "But first let's examine the data and compute a couple of recoded variables.\n",
    "\n",
    "The following function displays the values in a Series and the number of times each value appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values(series):\n",
    "    \"\"\"Count the values and sort.\n",
    "    \n",
    "    series: pd.Series\n",
    "    \n",
    "    returns: series mapping from values to frequencies\n",
    "    \"\"\"\n",
    "    return series.value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the values of `decile_score` which is the output of the COMPAS algorithm.  `1` is the lowest risk category; `10` is the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     1440\n",
       "2      941\n",
       "3      747\n",
       "4      769\n",
       "5      681\n",
       "6      641\n",
       "7      592\n",
       "8      512\n",
       "9      508\n",
       "10     383\n",
       "Name: decile_score, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values(cp['decile_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that COMPAS is not a binary classifier; that is, it does no predict that a defendant will or will not recidivate.  Rather, it gives each defendant a score that is intended to reflect the risk that they will recidivate.\n",
    "\n",
    "In order to evaluate the performance of COMPAS, the authors of the ProPublica article chose a threshold, `4`, and define decile scores at or below the threshold to be \"low risk\", and scores above the threshold to be \"high risk\".\n",
    "\n",
    "The choice of the threshold is arbitrary.  Later, we will see what happens with other choice, but we'll start by replicating the original analysis.\n",
    "\n",
    "`high_risk` is a Boolean Series that's `True` for respondents with a decile score greater than 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3897\n",
       "True     3317\n",
       "Name: HighRisk, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_risk = (cp['decile_score'] > 4)\n",
    "high_risk.name = 'HighRisk'\n",
    "values(high_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `two_year_recid` indicates whether a defendant was charged with another crime during a two year period, after the original charge, when they were not in a correctional facility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3963\n",
       "1    3251\n",
       "Name: two_year_recid, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values(cp['two_year_recid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`new_charge_2` is a Boolean Series that is `True` for defendants who were charged with another crime within two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3963\n",
       "True     3251\n",
       "Name: NewCharge2, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_charge_2 = (cp['two_year_recid'] == 1)\n",
    "new_charge_2.name = 'NewCharge2'\n",
    "values(new_charge_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make a cross-tabulation of `new_charge_2` and `high_risk`, the result is a DataFrame that indicates how many defendants are in each of four groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>HighRisk</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewCharge2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>False</td>\n",
       "      <td>2681</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>1216</td>\n",
       "      <td>2035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "HighRisk    False  True \n",
       "NewCharge2              \n",
       "False        2681   1282\n",
       "True         1216   2035"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(new_charge_2, high_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is called a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) or error matrix.  Reading from left to right and top to bottom, the elements of the matrix show the number of respondents who were:\n",
    "\n",
    "* Classified as low risk and not arrested again: there were 2681 *true negatives*; that is, cases where the test is negative (not high risk) and the prediction turned out to be true (not arrested again).\n",
    "\n",
    "* High risk and not arrested: there were 1282 *false positives*, cases where the test is positive and the prediction was false.\n",
    "\n",
    "* Low risk and arrested: there were 1216 *false negatives*, cases where the test is negative (low risk) and the prediction is false (the defendant was arrested again).\n",
    "\n",
    "* High risk and arrested: there were 2035 *true positives*, cases where the test is positive and the prediction was true.\n",
    "\n",
    "The values in this matrix are consistent with the values in the ProPublica article, so we can confirm that we are reading the data and replicating their analysis correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we'll see several ways to use the confusion matrix to quantify the accuracy of the test.\n",
    "\n",
    "But first let's compute confusion matrices for white and black defendants.\n",
    "\n",
    "Here are the values of `race`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "African-American    3696\n",
       "Asian                 32\n",
       "Caucasian           2454\n",
       "Hispanic             637\n",
       "Native American       18\n",
       "Other                377\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values(cp['race'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a Boolean Series that's true for white defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    4760\n",
       "True     2454\n",
       "Name: white, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white = (cp['race'] == 'Caucasian')\n",
    "white.name = 'white'\n",
    "values(white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the confusion matrix for white defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>HighRisk</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewCharge2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>False</td>\n",
       "      <td>1139</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>461</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "HighRisk    False  True \n",
       "NewCharge2              \n",
       "False        1139    349\n",
       "True          461    505"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(new_charge_2[white], high_risk[white])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these results are consistent with the table in the ProPublica article.\n",
    "\n",
    "`black` is a Boolean Series that is `True` for black defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3518\n",
       "True     3696\n",
       "Name: black, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black = (cp['race'] == 'African-American')\n",
    "black.name = 'black'\n",
    "values(black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the confusion matrix for black defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>HighRisk</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewCharge2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>False</td>\n",
       "      <td>990</td>\n",
       "      <td>805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "      <td>532</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "HighRisk    False  True \n",
       "NewCharge2              \n",
       "False         990    805\n",
       "True          532   1369"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(new_charge_2[black], high_risk[black])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again these results are consistent with the ProPublica article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arranging the confusion matrix\n",
    "\n",
    "In the previous section I arranged the confusion matrix to be consistent with the ProPublica article, to make it easy to check for consistency.\n",
    "\n",
    "In general, there are many possible ways to arrange a confusion matrix: you can put the predictions along the columns and the actual conditions along the rows, or the other way around, and you can sort the rows and columns in either order.\n",
    "\n",
    "There is no universal standard arrangement, but the one that seems to be most common is the one on the [confusion matrix Wikipedia page](https://en.wikipedia.org/wiki/Confusion_matrix), which looks like this:\n",
    "\n",
    "<img width=400, src='https://raw.githubusercontent.com/AllenDowney/ElementsOfDataScience/master/confusion_matrix1.png'>\n",
    "\n",
    "In this arrangement:\n",
    "\n",
    "* The predictions are along the rows.\n",
    "\n",
    "* The actual results are along the columns.\n",
    "\n",
    "* The rows and columns are sorted so true positives are in the upper left; true negatives are in the lower right.\n",
    "\n",
    "In the context of the ProPublica article:\n",
    "\n",
    "* \"Predicted condition positive\" means the defendant is classified as high risk\n",
    "\n",
    "* \"Predicted condition negative\" means low risk.\n",
    "\n",
    "* \"Condition positive\" means the defendant was arrested again (recidivated).\n",
    "\n",
    "* \"Condition negative\" means they were not arrested (survived).\n",
    "\n",
    "The following function make a confusion matrix with this arrangement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(cp, subgroup, threshold=4):\n",
    "    \"\"\"Make a confusion matrix.\n",
    "    \n",
    "    cp: DataFrame\n",
    "    subgroup: Boolean Series\n",
    "    \n",
    "    returns: DataFrame containing the confusion matrix\n",
    "    \"\"\"\n",
    "    a = np.where(cp['decile_score'] > threshold, \n",
    "                 'Positive', \n",
    "                 'Negative')\n",
    "    high_risk = pd.Series(a, name='Predicted')\n",
    "    \n",
    "    a = np.where(cp['two_year_recid'] == 1, \n",
    "                 'Condition', \n",
    "                 'No Condition')\n",
    "    new_charge_2 = pd.Series(a, name='Actual')\n",
    "\n",
    "    matrix = pd.crosstab(high_risk[subgroup], new_charge_2[subgroup])\n",
    "    matrix.sort_index(axis=0, ascending=False, inplace=True)\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the confusion matrix for white defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual</th>\n",
       "      <th>Condition</th>\n",
       "      <th>No Condition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Positive</td>\n",
       "      <td>505</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Negative</td>\n",
       "      <td>461</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual     Condition  No Condition\n",
       "Predicted                         \n",
       "Positive         505           349\n",
       "Negative         461          1139"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_white = make_matrix(cp, white)\n",
    "matrix_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual</th>\n",
       "      <th>Condition</th>\n",
       "      <th>No Condition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Positive</td>\n",
       "      <td>1369</td>\n",
       "      <td>805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Negative</td>\n",
       "      <td>532</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual     Condition  No Condition\n",
       "Predicted                         \n",
       "Positive        1369           805\n",
       "Negative         532           990"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_black = make_matrix(cp, black)\n",
    "matrix_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual</th>\n",
       "      <th>Condition</th>\n",
       "      <th>No Condition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Positive</td>\n",
       "      <td>2035</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Negative</td>\n",
       "      <td>1216</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual     Condition  No Condition\n",
       "Predicted                         \n",
       "Positive        2035          1282\n",
       "Negative        1216          2681"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_defendants = np.ones_like(white)\n",
    "matrix_all = make_matrix(cp, all_defendants)\n",
    "matrix_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Based on these results, how accurate is COMPAS as a binary classifier?  Well, it turns out that there are a *lot* of ways to answer that question.\n",
    "\n",
    "One of the simple ones is overall **accuracy** which is the fraction (or percentage) of correct predictions.\n",
    "\n",
    "To compute accuracy, it is convenient to extract from the confusion matrix the number of true negatives, false positives, false negatives, and true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn, tn = matrix_all.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of true predictions is `tp + tn`.\n",
    "\n",
    "The number of false predictions is `fp + fn`.\n",
    "\n",
    "So we can compute the fraction of true predictions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent(x, y):\n",
    "    \"\"\"Compute the percentage `x/(x+y)*100`.\n",
    "    \"\"\"\n",
    "    return x / (x+y) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.37288605489326"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = percent(tp + tn, fp + fn)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a way to evaluate a binary classifier, accuracy does not distinguish between true positives and true negatives, or false positives and false negatives.\n",
    "\n",
    "But it is often important to make these distinctions, because the benefits of true predictions and true negatives might be different, and the costs of false positives and false negatives might be different.\n",
    "\n",
    "### Predictive value\n",
    "\n",
    "One way to make these distinctions is to compute the \"predictive value\" of positive and negative tests. \n",
    "\n",
    "* **Positive predictive value (PPV)**, is the fraction of positive tests that are correct.\n",
    "\n",
    "* **Negative predictive value (NPV)**, is the fraction of negative tests that are correct.\n",
    "\n",
    "In this example, PPV is the fraction of high risk defendants who were arrested again.  NPV is the fraction of low risk defendants who were not arrested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a confusion matrix and computes these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_value(m):\n",
    "    \"\"\"Compute positive and negative predictive value.\n",
    "    \n",
    "    m: confusion matrix\n",
    "    \"\"\"\n",
    "    tp, fp, fn, tn = m.to_numpy().flatten()\n",
    "    ppv = percent(tp, fp)\n",
    "    npv = percent(tn, fn)\n",
    "    return ppv, npv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute predictive value for all defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61.350618028338864, 68.79651013600206)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppv, npv = predictive_value(matrix_all)\n",
    "ppv, npv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all defendants, a negative test result is correct about 69% of the time; a positive test is correct about 61% of the time.\n",
    "\n",
    "## Sensitivity and specificity\n",
    "\n",
    "Another way to characterize the accuracy of a test is to compute \n",
    "\n",
    "* **Sensitivity**, which is the probability of predicting correctly when the condition is present, and \n",
    "\n",
    "* **Specificity**, which is the probability of predicting correctly when the condition is absent.\n",
    "\n",
    "A test is \"sensitive\" if it correctly detects the positive condition. \n",
    "In this example, sensitivity is the fraction of recidivists who were classified as high risk.\n",
    "\n",
    "And a test is \"specific\" if it correctly identifies the negative condition.  In this example, specificity is the fraction of non-recidivists who we classified as low risk.\n",
    "\n",
    "The following function takes a confusion matrix and computes sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sens_spec(m):\n",
    "    \"\"\"Compute sensitivity and specificity.\n",
    "    \n",
    "    m: confusion matrix\n",
    "    \"\"\"\n",
    "    tp, fp, fn, tn = m.to_numpy().flatten()\n",
    "    sens = percent(tp, fn)\n",
    "    spec = percent(tn, fp)\n",
    "    return sens, spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are sensitivity and specificity for all defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62.59612426945556, 67.65076961897553)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens, spec = sens_spec(matrix_all)\n",
    "sens, spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we evaluate COMPAS as a binary classifier, it is a little more specific than sensitive:\n",
    "\n",
    "* About 64% of the recidivists were classified as high risk.\n",
    "\n",
    "* About 67% of the non-recidivists were classified as low risk.\n",
    "\n",
    "It can be hard to keep all of these metrics straight, especially when you are learning about them for the first time.  This following diagram from Wikipedia might help:\n",
    "\n",
    "<img width=800, src='https://raw.githubusercontent.com/AllenDowney/ElementsOfDataScience/master/confusion_matrix2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPV and sensitivity are similar in the sense that they both have true positives in the numerator.  The difference is the denominator:\n",
    "\n",
    "* PPV is the ratio of true positives to all positive tests.  So it answers the question, \"Of all positive tests, how many are correct?\"\n",
    "\n",
    "* Sensitivity is the ratio of true positives to all positive conditions.  So it answers the question \"Of all positive conditions, how many are classified correctly?\"\n",
    "\n",
    "Similary, NPV and sensitivity both have true negatives in the numerator, but:\n",
    "\n",
    "* NPV is the ratio of true negatives to all negative tests.  It answers, \"Of all negative tests, how many are correct?\"\n",
    "\n",
    "* Specificiy is the ratio of true negatives to all negative conditions.  It answers, \"Of all negative conditions, how many are classified correctly?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False positive and negative rates\n",
    "\n",
    "The ProPublica article reports PPV and NPV, but instead of sensitivity and specificity, it reports:\n",
    "\n",
    "* **False positive rate**, which is the ratio of false positives to all negative conditions.  It answers, \"Of all negative conditions, how many are misclassified?\" \n",
    "\n",
    "* **False negative rate**, which is the ratio of false negatives to all positive conditions.  It answers, \"Of all positive conditions, how many are misclassified?\"\n",
    "\n",
    "In this example:\n",
    "\n",
    "* The false positive rate is the fraction of non-recidivists who were classified as high risk.\n",
    "\n",
    "* The false negative rate is the fraction of recidivists who were classified as low risk.\n",
    "\n",
    "The following function takes a confusion matrix and computes false positive and false negative rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rates(m):\n",
    "    \"\"\"Compute false positive and false negative rate.\n",
    "    \n",
    "    m: confusion matrix\n",
    "    \"\"\"\n",
    "    tp, fp, fn, tn = m.to_numpy().flatten()\n",
    "    fpr = percent(fp, tn)\n",
    "    fnr = percent(fn, tp)\n",
    "    return fpr, fnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the error rates for all defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.349230381024476, 37.40387573054445)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, fnr = error_rates(matrix_all)\n",
    "fpr, fnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive rate is the complement of specificity, which means they have to add up to 100%\n",
    "\n",
    "And false negative rate is the complement of sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100.0, 100.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr + spec, fnr + sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So FPR and FNR are just another way of reporting sensitivity and specificity.\n",
    "\n",
    "In general, I think it is better to report sensitivity and specificity, because\n",
    "\n",
    "* I think the positive framing is easier to interpret than the negative framing, and\n",
    "\n",
    "* I find it easier to remember what \"sensitivity\" and \"specificity\" mean.\n",
    "\n",
    "I think \"false positive rate\" and \"false negative rate\" are more problematic.  For example, \"false positive rate\" could just as easily mean either\n",
    "\n",
    "1. The fraction of positive tests that are incorrect, or\n",
    "\n",
    "2. The fraction of negative conditions that are misclassified.\n",
    "\n",
    "It is only a convention that the first is called the \"false discovery rate\" and the second is the \"false positive rate\".  I suspect I am not the only one that gets them confused.\n",
    "\n",
    "So, here's my recommendation: if you have the choice, generally use PPV, NPV, sensitivity and specificity, and avoid the other metrics.\n",
    "\n",
    "However, since the ProPublica article uses FPR and FNR, I will too.\n",
    "\n",
    "They also report LR+ and LR-, but those are just combinations of other metrics and not relevant to the current discussion, so I will ignore them.\n",
    "\n",
    "However, there is one other metric that I think is relevant, and not included in the ProPublica tables: prevalence, which is the fraction of all cases where the condition is positive.  In the example, it's the fraction of defendants who recidivate.\n",
    "\n",
    "The following function computes prevalence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevalence(df):\n",
    "    \"\"\"Compute prevalence.\n",
    "    \n",
    "    m: confusion matrix\n",
    "    \"\"\"\n",
    "    tp, fp, fn, tn = df.to_numpy().flatten()\n",
    "    prevalence = percent(tp+fn, tn+fp)\n",
    "    return prevalence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the prevalence for all defendants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.06515109509287"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev = prevalence(matrix_all)\n",
    "prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 45% of the defendants in this dataset were charged with another crime within two years of their release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All metrics\n",
    "\n",
    "The following function takes a confusion matrix and computes the metrics from the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(m, name=''):\n",
    "    \"\"\"Compute all metrics.\n",
    "    \n",
    "    m: confusion matrix\n",
    "    \n",
    "    returns: DataFrame\n",
    "    \"\"\"\n",
    "    fpr, fnr = error_rates(m)\n",
    "    ppv, npv = predictive_value(m)\n",
    "    prev = prevalence(m)\n",
    "    \n",
    "    index = ['FP rate', 'FN rate', 'PPV', 'NPV', 'Prevalence']\n",
    "    df = pd.DataFrame(index=index, columns=['Percent'])\n",
    "    df.Percent = fpr, fnr, ppv, npv, prev\n",
    "    df.index.name = name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics for all defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All defendants</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>FP rate</td>\n",
       "      <td>32.349230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FN rate</td>\n",
       "      <td>37.403876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PPV</td>\n",
       "      <td>61.350618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NPV</td>\n",
       "      <td>68.796510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prevalence</td>\n",
       "      <td>45.065151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Percent\n",
       "All defendants           \n",
       "FP rate         32.349230\n",
       "FN rate         37.403876\n",
       "PPV             61.350618\n",
       "NPV             68.796510\n",
       "Prevalence      45.065151"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(matrix_all, 'All defendants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these results to the table from ProPublica, it looks like our analysis agrees with theirs.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/AllenDowney/ElementsOfDataScience/master/machine_bias_table.png'>\n",
    "\n",
    "\n",
    "We can compute the same metrics for black defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black defendants</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>FP rate</td>\n",
       "      <td>44.846797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FN rate</td>\n",
       "      <td>27.985271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PPV</td>\n",
       "      <td>62.971481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NPV</td>\n",
       "      <td>65.045992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prevalence</td>\n",
       "      <td>51.433983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Percent\n",
       "Black defendants           \n",
       "FP rate           44.846797\n",
       "FN rate           27.985271\n",
       "PPV               62.971481\n",
       "NPV               65.045992\n",
       "Prevalence        51.433983"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(matrix_black, 'Black defendants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for white defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White defendants</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>FP rate</td>\n",
       "      <td>23.454301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FN rate</td>\n",
       "      <td>47.722567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PPV</td>\n",
       "      <td>59.133489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NPV</td>\n",
       "      <td>71.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prevalence</td>\n",
       "      <td>39.364303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Percent\n",
       "White defendants           \n",
       "FP rate           23.454301\n",
       "FN rate           47.722567\n",
       "PPV               59.133489\n",
       "NPV               71.187500\n",
       "Prevalence        39.364303"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(matrix_white, 'White defendants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these results are consistent with those reported in the article, including the headline results:\n",
    "\n",
    "1. The false positive rate for black defendants is substantially higher than for white defendants (45%, compared to 23%).\n",
    "\n",
    "2. The false negative rate for black defendants is substantially lower (28%, compared to 48%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.84679665738162, 27.985270910047344)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rates(matrix_black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.45430107526882, 47.72256728778468)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rates(matrix_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Of all people who *will* recidivate, black defendants are more likely than white defendants to be classified as high risk, and (if these classifications influence sentencing decisions) more likely to be sent to prison.  \n",
    "\n",
    "* Of all people who *will not* recidivate, white defendants are less likely than black defendants to be be classified as high risk and sent to prison.\n",
    "\n",
    "This seems clearly unfair, but it turns out that \"fair\" is complicated.  After the ProPublica article, the Washington Post published a response by Sam Corbett-Davies, Emma Pierson, Avi Feller and Sharad Goel: \"[A computer program used for bail and sentencing decisions was labeled biased against blacks. Its actually not that clear.](https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/)\"\n",
    "\n",
    "I encourage you to read that article, and then run [the next notebook](), where we will unpack their arguments and replicate their analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
