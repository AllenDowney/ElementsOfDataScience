{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and the k-means Algorithm\n",
    "\n",
    "Copyright 2020 Allen B. Downey\n",
    "\n",
    "License: [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook introduces [cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis) and one of the most common algorithms for it, [k-means](https://en.wikipedia.org/wiki/K-means_clustering).\n",
    "\n",
    "We'll proceed \"top-down\"; that is, we'll use scikit-learn first, then we'll open the hood and see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell imports the libraries we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines a function we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate(**options):\n",
    "    \"\"\"Decorate the current axes.\n",
    "    \n",
    "    Call decorate with keyword arguments like\n",
    "    decorate(title='Title',\n",
    "             xlabel='x',\n",
    "             ylabel='y')\n",
    "             \n",
    "    The keyword arguments can be any of the axis properties\n",
    "    https://matplotlib.org/api/axes_api.html\n",
    "    \"\"\"\n",
    "    ax = plt.gca()\n",
    "    ax.set(**options)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        ax.legend(handles, labels)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Cluster analysis is a set of tools for looking at data and \n",
    "\n",
    "* Discovering groups, species, or categories,\n",
    "\n",
    "* Defining boundaries between groups.\n",
    "\n",
    "It is a form of \"unsupervised\" learning, which means that the only input is the dataset itself; the algorithm is not given any correct examples to learn from.\n",
    "\n",
    "As an example, I'll used data collected and made available by Dr. Kristen Gorman at the Palmer Long-Term Ecological Research Station in Antarctica.\n",
    "\n",
    "This dataset was published to support this article: Gorman, Williams, and Fraser, [\"Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus *Pygoscelis*)\"](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081), March 2014.\n",
    "\n",
    "The following cell downloads the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data files from https://github.com/allisonhorst/palmerpenguins\n",
    "# With gratitude to Allison Horst (@allison_horst)\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('penguins_raw.csv'):\n",
    "    !wget https://github.com/allisonhorst/palmerpenguins/raw/master/inst/extdata/penguins_raw.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is stored in a CSV file, which contains one row for each penguin and one column for each variable.\n",
    "\n",
    "I'll use Pandas to read the CSV file and put the results in a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('penguins_raw.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataFrame` is like a 2-D array, but it also contains names for the columns and labels for the rows.\n",
    "\n",
    "The `shape` of the `DataFrame` is the number of rows and columns.\n",
    "\n",
    "The `head` method displays the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three species of penguins are represented in the dataset: Adelie, Chinstrap and Gentoo, as shown in this illustration (by Allison Horst, available under the [CC-BY](https://creativecommons.org/licenses/by/2.0/) license):\n",
    "\n",
    "<img width=\"400\" src=\"https://pbs.twimg.com/media/EaAWkZ0U4AA1CQf?format=jpg&name=4096x4096\">\n",
    "\n",
    "In this dataset we are told that there are three species, and we are told which species each penguin belongs to.\n",
    "But for purposes of clustering, we'll pretend we don't have this information and we'll see whether the algorithm \"discovers\" the different species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurements we'll use are:\n",
    "\n",
    "* Body Mass in grams (g).\n",
    "\n",
    "* Flipper Length in millimeters (mm).\n",
    "\n",
    "* Culmen Length in millimeters.  \n",
    "\n",
    "* Culmen Depth in millimeters.\n",
    "\n",
    "If you are not familiar with the word \"culmen\", it refers to the [top margin of the beak](https://en.wikipedia.org/wiki/Bird_measurement#Culmen), as shown in the following illustration (also by Allison Horst):\n",
    "\n",
    "<img width=\"400\" src=\"https://pbs.twimg.com/media/EaAXQn8U4AAoKUj?format=jpg&name=4096x4096\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might seem like an artificial exercise.  If we already know that there are three species, why are we trying to discover them?\n",
    "\n",
    "For now, I'll just say that it's a learning example.  But let's come back to this question: what is unsupervised clustering good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of measurements\n",
    "\n",
    "The measurements we have will be most useful for clustering if there are substantial differences between species and small variation within species.  To see whether that is true, and to what degree, I will plot distributions of measurements for each species. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, I'll create a new column, called `Species2`, that contains a shorter version of the species names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten(species):\n",
    "    \"\"\"Select the first word from a string.\"\"\"\n",
    "    return species.split()[0]\n",
    "\n",
    "df['Species2'] = df['Species'].apply(shorten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the `groupby` method to divide the dataset by species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('Species2')\n",
    "type(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a `GroupBy` object that contains the three groups and their names.  The following loop prints the group names and the number of penguins in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in grouped:\n",
    "    print(name, len(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `GroupBy` object to extract a column, like flipper length, from each group and compute its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'Flipper Length (mm)'\n",
    "\n",
    "for name, group in grouped:\n",
    "    print(name, group[varname].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use it to display the distribution of values in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in grouped:\n",
    "    sns.kdeplot(group[varname], label=name)\n",
    "    \n",
    "decorate(xlabel=varname,\n",
    "         ylabel='PDF',\n",
    "         title='Distributions of features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kdeplot` uses [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) to make a smooth histogram of the values.\n",
    "\n",
    "It looks like we can use flipper length to identify Gentoo penguins, but not to distinguish the other two species.\n",
    "\n",
    "To make these steps easier to reuse, I'll wrap them a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kdeplots(df, varname):\n",
    "    \"\"\"Make a KDE plot for each species.\n",
    "    \n",
    "    df: DataFrame\n",
    "    varname: string column name\n",
    "    by: string column name\n",
    "    \n",
    "    returns: dictionary from species name to Cdf\n",
    "    \"\"\"\n",
    "    grouped = df.groupby('Species2')\n",
    "    for name, group in grouped:\n",
    "        sns.kdeplot(group[varname], label=name)\n",
    "    \n",
    "    decorate(xlabel=varname,\n",
    "         ylabel='PDF',\n",
    "         title='Distributions of features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use it to explore other features, like culmen length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_kdeplots(df, 'Culmen Length (mm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we can use culmen length to identify Adelie penguins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use `make_kdeplots` to display the distributions of one of the other two features:\n",
    "\n",
    "* `'Body Mass (g)'`\n",
    "* `'Culmen Depth (mm)'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot\n",
    "\n",
    "If we can identify Gentoo penguins by flipper length and Adelie penguins by culmen length, maybe we can combine these variables to identify all three species. \n",
    "\n",
    "I'll start by making a scatter plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = 'Flipper Length (mm)'\n",
    "var2 = 'Culmen Length (mm)'\n",
    "var3 = 'Culmen Depth (mm)'\n",
    "var4 = 'Body Mass (g)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in grouped:\n",
    "    plt.plot(group[var1], group[var2], \n",
    "             'o', alpha=0.4, label=name)\n",
    "    \n",
    "decorate(xlabel=var1, ylabel=var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those two features, we can divide the penguins into clusters with not much overlap.\n",
    "\n",
    "We're going to make lots of scatter plots, so let's wrap that code in a function.\n",
    "\n",
    "And we'll generalize it to take `by` as a parameter, so we can group by any column, not just `Species2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(df, var1, var2, by):\n",
    "    \"\"\"Make a scatter plot.\n",
    "    \n",
    "    df: DataFrame\n",
    "    var1: string column name, x-axis\n",
    "    var2: string column name, y-axis\n",
    "    by: string column name, groupby\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(by)\n",
    "    for name, group in grouped:\n",
    "        plt.plot(group[var1], group[var2], \n",
    "                 'o', alpha=0.4, label=name)\n",
    "    \n",
    "    decorate(xlabel=var1, ylabel=var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a scatter plot of flipper and culmen length for the three species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot(df, var1, var2, 'Species2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Make a scatter plot using any other pair of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of these scatter plots as 2-D views of a 4-D feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the labels\n",
    "\n",
    "Now, let's pretend we don't know anything about the different species, and we'll see whether we can rediscover these clusters.\n",
    "\n",
    "To see what the problem looks like, I'll add a column of labels to the `DataFrame` and set it to 0 for all penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we group by label, there's only one big cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot(df, var1, var2, 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we run k-means clustering on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "First I'll use the implementation of k-means in scikit-learn; then we'll write our own.\n",
    "\n",
    "In the dataset, we have 344 penguins and 19 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But some of the variables are NaN, which indicates missing data.\n",
    "\n",
    "So I'll use `dropna` to drop any rows that have missing data for the two features we're going to use, flipper length and culmen length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [var1, var2]\n",
    "data = df.dropna(subset=features).copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll extract just those two columns as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = data[features].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `KMeans` to identify the clusters.\n",
    "\n",
    "`n_clusters` indicates how many cluster we want; this parameter is the $k$ the algorithm is named for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3).fit(M)\n",
    "type(kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an object that contains \n",
    "\n",
    "* Labels that indicates which cluster each penguin is assigned to, and\n",
    "\n",
    "* The centers of the clusters.\n",
    "\n",
    "I'll store the labels as a columns in `data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = kmeans.labels_\n",
    "data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That way we can use `scatterplot` to show the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot(data, var1, var2, 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `KMeans` object also contains the centers of the clusters as coordinate pairs in a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the centers, I'll transpose the array and assign the columns to `x` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = np.transpose(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll plot the centers with x's and o's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(color='C3', ls='none', mfc='none')\n",
    "plt.plot(xs, ys, marker='o', ms=15, **options)\n",
    "plt.plot(xs, ys, marker='x', ms=10, **options);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let wrap that up in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_centers(centers, color='C3'):\n",
    "    \"\"\"Plot cluster centers.\n",
    "    \n",
    "    centers: array with x and y columns\n",
    "    color: string color specification\n",
    "    \"\"\"\n",
    "    xs, ys = np.transpose(centers)\n",
    "    options = dict(color=color, ls='none', mfc='none')\n",
    "    plt.plot(xs, ys, marker='o', ms=15, **options)\n",
    "    plt.plot(xs, ys, marker='x', ms=10, **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pull it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot(data, var1, var2, 'labels')\n",
    "plot_centers(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows the data, color-coded by assigned label, and the centers of the clusters.\n",
    "\n",
    "It looks like k-means does a reasonable job of rediscovering the species, but with some confusion between Adelie (lower left) and Chinstrap (top center).\n",
    "\n",
    "As a reminder, here are the right answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot(data, var1, var2, 'Species2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the color coding for the clusters is not consistent because the centers we get from k-means are in a random order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Here's the code from this section all in one place.  Modify it to use any two features and see what the results look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = [var1, var2]\n",
    "\n",
    "data2 = df.dropna(subset=features2).copy()\n",
    "\n",
    "M2 = data2[features2].to_numpy()\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=3).fit(M2)\n",
    "\n",
    "data2['labels'] = kmeans2.labels_\n",
    "\n",
    "scatterplot(data2, var1, var2, 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing k-means\n",
    "\n",
    "Now let's see how the algorithm works.  At a high level, there are three steps:\n",
    "\n",
    "1. Choose $k$ random points in the dataset as initial centers.\n",
    "2. Assign each point in the dataset to the closest center.\n",
    "3. Compute new centers by calculating the \"center of mass\" in each cluster.\n",
    "\n",
    "Then you repeat steps 2 and 3 until the centers stop moving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select random points from the dataset, I'll use `np.random.choice` to select three indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(len(M), size=3)\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then use the indices to select rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = M[index]\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll wrap that in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_start(M, k):\n",
    "    \"\"\"Choose k random elements of M.\n",
    "    \n",
    "    M: NumPy array with rows of coordinates\n",
    "    k: number of centers to choose\n",
    "    \n",
    "    returns: NumPy array\n",
    "    \"\"\"\n",
    "    index = np.random.choice(len(M), size=k)\n",
    "    centers = M[index]\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = choose_random_start(M, 3)\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what the centers look like on the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = 0\n",
    "scatterplot(data, var1, var2, 'labels')\n",
    "plot_centers(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to assign each point to the closest center.  So we need to compute the distance between each point and each of the centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distances\n",
    "\n",
    "To demonstrate the process, I'll pick just one of the centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_x, center_y = centers[0]\n",
    "center_x, center_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it will be convenient to have the `x` and `y` coordinates in separate arrays.  I can do that with `np.transpose`, which turns the columns into rows; then I can assign the rows to `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.transpose(M)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the x-axis, the distance from each point to this center is `x-center_x`.\n",
    "\n",
    "Along the y-axis, the distance is `y-center_y`.\n",
    "\n",
    "The distance from each point to the center is the hypotenuse of the triangle, which I can compute with `np.hypot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.hypot(x-center_x, y-center_y)\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array that contains the distance from each point to the chosen center.\n",
    "\n",
    "To see if we got it right, I'll plot the center and the points, with the size of the points proportional to distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(center_x, center_y, 'rx', markersize=10)\n",
    "plt.scatter(x, y, s=distances)\n",
    "\n",
    "decorate(xlabel=var1, ylabel=var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least visually, it seems like the size of the points is proportional to their distance from the center.\n",
    "\n",
    "So let's put those steps into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(M, center):\n",
    "    \"\"\"Compute distances to the given center.\n",
    "    \n",
    "    M: NumPy array of coordinates\n",
    "    center: x, y coordinates of the center\n",
    "    \n",
    "    returns: NumPy array of float distances\n",
    "    \"\"\"\n",
    "    x, y = np.transpose(M)\n",
    "    center_x, center_y = center\n",
    "    distances = np.hypot(x-center_x, y-center_y)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function to make a list of distance arrays, one for each center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_arrays = [compute_distances(M, center)\n",
    "                   for center in centers]\n",
    "len(distance_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling the points\n",
    "\n",
    "The next step is to label each point with the index of the center it is closest to.\n",
    "\n",
    "`distance_arrays` is a list of arrays, but we can convert it to a 2-D array like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array(distance_arrays)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A` has one row for each center and one column for each point.\n",
    "\n",
    "Now we can use `np.argmin` to find the shortest distance in each column and return its index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = np.argmin(A, axis=0)\n",
    "data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array of indices in the range `0..2`, which we assign to a column in `data`.\n",
    "\n",
    "Let's put these steps in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_labels(M, centers):\n",
    "    \"\"\"Label each point with the index of the closest center.\n",
    "    \n",
    "    M: NumPy array of coordinates\n",
    "    centers: array of coordinates for the centers\n",
    "    \n",
    "    returns: array of labels, 0..k-1\n",
    "    \"\"\"\n",
    "    distance_arrays = [compute_distances(M, center)\n",
    "                       for center in centers]\n",
    "    A = np.array(distance_arrays)\n",
    "    labels = np.argmin(A, axis=0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = compute_labels(M, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot(data, var1, var2, 'labels')\n",
    "plot_centers(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we get lucky, we might start with one point near the center of each cluster.\n",
    "But even if we are unlucky, we can improve the results by recentering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find new centers\n",
    "\n",
    "The last step is to use the labels from the previous step to compute the center of each cluster.\n",
    "\n",
    "I'll start by using `groupby` to group the points by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby('labels')\n",
    "for name, group in grouped:\n",
    "    print(name, len(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `GroupBy` object to select the columns we're using and compute their means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('labels')[features].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a `DataFrame` that contains the central coordinates of each cluster.\n",
    "\n",
    "I'll put these steps in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_centers(data, features):\n",
    "    \"\"\"Compute the center of each cluster.\n",
    "    \n",
    "    data: DataFrame\n",
    "    features: list of string column names\n",
    "    \"\"\"\n",
    "    means = data.groupby('labels')[features].mean()\n",
    "    return means.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value is a NumPy array that contains the new centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centers = compute_new_centers(data, features)\n",
    "new_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it looks like with the old centers in gray and the new centers in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot(data, var1, var2, 'labels')\n",
    "plot_centers(centers, color='gray')\n",
    "plot_centers(new_centers, color='C3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The k-means algorithm\n",
    "\n",
    "Now here's the whole algorithm in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(data, features, k):\n",
    "    \"\"\"Cluster by k means.\n",
    "    \n",
    "    data: DataFrame\n",
    "    features: list of string column names\n",
    "    k: number of clusters\n",
    "    \n",
    "    returns: array of centers\n",
    "    \"\"\"\n",
    "    M = data[features].to_numpy()\n",
    "    centers = choose_random_start(M, k)\n",
    "\n",
    "    for i in range(15):\n",
    "        data['labels'] = compute_labels(M, centers)\n",
    "        centers = compute_new_centers(data, features)\n",
    "\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what the results look like after 15 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = k_means(data, features, 3)\n",
    "scatterplot(data, var1, var2, 'labels')\n",
    "plot_centers(centers, color='C3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are (as far as I can see) identical to what we got from the scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(M)\n",
    "data['labels'] = kmeans.labels_\n",
    "\n",
    "scatterplot(data, var1, var2, 'labels')\n",
    "plot_centers(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation\n",
    "\n",
    "Here's an animation that shows the algorithm in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "interval = 1\n",
    "\n",
    "centers = choose_random_start(M, k=3)\n",
    "plt.figure()\n",
    "\n",
    "for i in range(10):\n",
    "    # label and scatter plot\n",
    "    data['labels'] = compute_labels(M, centers)\n",
    "    scatterplot(data, var1, var2, 'labels')\n",
    "    plot_centers(centers, color='gray')\n",
    "        \n",
    "    # compute new centers and plot them\n",
    "    new_centers = compute_new_centers(data, features)        \n",
    "    plot_centers(new_centers)\n",
    "    centers = new_centers\n",
    "        \n",
    "    # show the plot, wait, and clear\n",
    "    plt.show()\n",
    "    sleep(interval)        \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Run the previous cell a few times.  Do you always get the same clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of clusters\n",
    "\n",
    "All of this is based on the assumption that you know how many clusters you are looking for, which is true for some applications, but not always.\n",
    "\n",
    "Let's see what goes wrong if you ask for too many clusters, or too few.\n",
    "\n",
    "**Exercise:** Run the following code with different values of `n_clusters` and see what the results look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(M)\n",
    "data['labels'] = kmeans.labels_\n",
    "\n",
    "scatterplot(data, var1, var2, 'labels')\n",
    "plot_centers(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarization\n",
    "\n",
    "One of the problems with the results we have seen so far is that the lines between the clusters are mostly vertical.\n",
    "\n",
    "That's because the range of values is wider for flipper length than culmen length, about 60 mm compared to 28 mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.max(axis=0) - M.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute the distance from each point to each center, the distances in the $x$ direction tend to dominate.\n",
    "\n",
    "This is a common problem with algorithms that are based on distance in multidimensional space.\n",
    "\n",
    "It is such a common problem that there is a common solution: [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling).\n",
    "\n",
    "The goal of feature scaling is to transform the features so the distances along each axis are comparable.\n",
    "\n",
    "One version of feature scaling is \"standardization\", which consists of\n",
    "\n",
    "1. Subtracting the mean from each feature, and\n",
    "2. Dividing through by the standard deviation.\n",
    "\n",
    "Here's how we can do it with the features in `M`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = M.mean(axis=0)\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = M.std(axis=0)\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_std = (M - means) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we run the algorithm again with standardized features.\n",
    "\n",
    "Notice that I have to transform the centers back before plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(M_std)\n",
    "data['labels'] = kmeans.labels_\n",
    "\n",
    "scatterplot(data, var1, var2, 'labels')\n",
    "\n",
    "centers = kmeans.cluster_centers_ * stds + means\n",
    "plot_centers(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a lot better!  Again, here are the actual species for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatterplot(data, var1, var2, 'Species2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provides `StandardScaler`, which does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(M)\n",
    "M_std = scaler.transform(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `scaler` provides `inverse_transform`, which we can use to transform the centers back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(M_std)\n",
    "data['labels'] = kmeans.labels_\n",
    "\n",
    "scatterplot(data, var1, var2, 'labels')\n",
    "\n",
    "centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "plot_centers(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The k-means algorithm does unsupervised clustering, which means that we don't tell it where the clusters are; we just provide the data and ask it to find a given number of clusters.\n",
    "\n",
    "In this notebook, we asked it to find clusters in a group of penguins based on two features, flipper length and culmen length.  The clusters it finds reflect the species in the dataset, especially if we standardize the data.\n",
    "\n",
    "In this example we used only two features, because that makes it easy to visualize the results.  But k-means extends easily to any number of dimensions (see the exercise below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is this good for?\n",
    "\n",
    "Well, [Wikipedia provides this list of applications](https://en.wikipedia.org/wiki/Cluster_analysis#Applications).  Applying clustering analysis to these applications, I see a few general ideas:\n",
    "\n",
    "* From an engineering point of view, clustering can be used to automate some kinds of analysis people do, which might be faster, more accurate, or less expensive.  And it can work with large datasets and high numbers of dimensions that people can't handle.\n",
    "\n",
    "* From a scientific point of view, clustering provides a way to test whether the patterns we see are in the data or in our minds.\n",
    "\n",
    "This second point is related to old philosophical questions about the [nature of categories](https://plato.stanford.edu/entries/natural-kinds/).  Putting things into categories seems to be a natural part of how humans think, but we have to wonder whether the categories we find truly \"carve nature at its joints\", as [Plato put it](https://mitpress.mit.edu/books/carving-nature-its-joints).\n",
    "\n",
    "If a clustering algorithm finds the same \"joints\" we do, we might have more confidence they are not entirely in our minds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use the scikit-learn implementation of k-means to find clusters using all four features (flipper length, culmen length and depth, body mass).  How do the results compare to what we got with just two features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
